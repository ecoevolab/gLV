{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Section: Mount-cluster\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "remote = \"/mnt/data/sur/users/mrivera\"\n",
    "mount_p = \"/home/mriveraceron/fenix_mount\"\n",
    "\n",
    "# Mount only if not already mounted\n",
    "if os.path.ismount(mount_p):\n",
    "    print(\">> Cluster is already mounted\")\n",
    "else:\n",
    "    subprocess.run([\n",
    "        'sshfs',\n",
    "        '-o', 'ro',   # <-- read-only option\n",
    "        f'mrivera@fenix.lavis.unam.mx:{remote}',\n",
    "        mount_p\n",
    "    ], capture_output=True, text=True)\n",
    "    print(\">> Cluster mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1f135",
   "metadata": {},
   "source": [
    "We defining seeding for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c04b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import random \n",
    "\n",
    "# Define function for seeding\n",
    "def set_seed(seed=42):\n",
    "    # Set ALL seeds for full reproducibility\n",
    "    torch.manual_seed(seed)                 # Seed CPU \n",
    "    torch.cuda.manual_seed(seed)            # Seed GPU\n",
    "    np.random.seed(seed)                    # Seed numpy\n",
    "    random.seed(seed)                       # Seed python random\n",
    "    torch.backends.cudnn.deterministic = True   # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False   \n",
    "\n",
    "set_seed(seed=54)  # Ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d6b8b",
   "metadata": {},
   "source": [
    "Define paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a974ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Section: Generate-paths\n",
    "# Target-path\n",
    "exp = \"c748247a-8dc2\"\n",
    "# A_dir = os.path.join(mount_p, f\"Experiments/{exp}/A-mat\")\n",
    "# tgt_dir = os.path.join(mount_p, f\"Experiments/{exp}/Replica2/GNN-targets\")\n",
    "# data_path = os.path.join(mount_p, f\"Data/{exp}.tsv\")\n",
    "\n",
    "exp_dir = \"/home/mriveraceron/fenix_mount/Train-sims/4379fd40-9f0a\"\n",
    "A_dir = os.path.join(exp_dir, \"A-mat\")\n",
    "tgt_dir = os.path.join(exp_dir, \"GNN-targets\")\n",
    "odes_path = os.path.join(exp_dir, \"raw-ODEs\")\n",
    "data_path = os.path.join(exp_dir, \"parameters-sims.tsv\")\n",
    "\n",
    "# Generate ID for training.\n",
    "timeID = datetime.now().strftime(\"Y%YM%mD%d\")\n",
    "\n",
    "#  Load-data\n",
    "data = pd.read_csv(data_path, sep=\"\\t\")             # Load data\n",
    "data_ids = data['id'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f433ddc",
   "metadata": {},
   "source": [
    "First, we define a loader function that takes the simulation ID, reads the interaction matrix, and converts it into edge weights. Then, it reads the targets, converts them into a tensor, and finally returns a Data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Load-function\n",
    "from torch_geometric.data import Data\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "def load_single_data(id, A_dir, tgt_path):\n",
    "    # Load adjacency matrix \n",
    "    A_path = os.path.join(A_dir, f\"A_{id}.feather\")\n",
    "    A = pd.read_feather(A_path).to_numpy(dtype=np.float32)\n",
    "    # Vector of edge weights\n",
    "    row_idx, col_idx = np.nonzero(A)\n",
    "    edge_weights = A[row_idx, col_idx]\n",
    "    # Convert to torch tensors efficiently\n",
    "    edge_index = torch.from_numpy(np.vstack([row_idx, col_idx]).astype(np.int64))\n",
    "    edge_weights = torch.from_numpy(edge_weights)\n",
    "    # Load target features \n",
    "    tgt_path = os.path.join(tgt_dir, f\"tgt_{id}.feather\")\n",
    "    tgt_table =  feather.read_table(tgt_path, columns=['K_s'])\n",
    "    y_tensor = torch.from_numpy(tgt_table.to_pandas().to_numpy(dtype=np.float32))   \n",
    "    # Node features - simple ones vector\n",
    "    n = A.shape[0]\n",
    "    x_tensor = torch.ones(n, 1, dtype=torch.float32)\n",
    "    # Clean up large intermediate\n",
    "    del A, tgt_table\n",
    "    # Create Data object\n",
    "    data = Data(\n",
    "        x=x_tensor,\n",
    "        edge_weights=edge_weights,\n",
    "        edge_index=edge_index,\n",
    "        y=y_tensor\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca6d2a",
   "metadata": {},
   "source": [
    "Divide data into training and validation set.\n",
    "Testing: Can loading time of data be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Divide-data\n",
    "import random\n",
    "\n",
    "# Load all data samples (for demo, we use only first 100 samples)\n",
    "indices = list(range(1, len(data_ids)))  # Indices 1-100\n",
    "random.shuffle(indices)  # Uses Python's random module (already seeded)\n",
    "\n",
    "# Now select first 80 for training, rest for validation\n",
    "indx = round(len(indices) * .8)\n",
    "train_indices = indices[:indx]            # First 80 shuffled indices\n",
    "val_indices = indices[indx:]              # Last 20 shuffled indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Declare new function\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "def generate_data_parallel(idx, A_dir, tgt_dir, num_workers=4):  # idx is a list\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        data_list = list(executor.map(\n",
    "            load_single_data,           \n",
    "            idx,                       # List of IDs to iterate over\n",
    "            [A_dir]*len(idx),          # Repeat A_dir for each ID\n",
    "            [tgt_dir]*len(idx)         # Repeat tgt_dir for each ID\n",
    "        ))\n",
    "    return data_list\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Generate data with method 1\n",
    "x = train_indices[:1000]\n",
    "\n",
    "start = time.time()\n",
    "data = [load_single_data(data_ids[id], A_dir, tgt_dir) for id in x] \n",
    "not_par_time = time.time() - start\n",
    "\n",
    "# Generate data with method 2\n",
    "start = time.time()\n",
    "batch_size = 100\n",
    "batching_ids = data_ids[x]\n",
    "num_batches = (len(batching_ids) + batch_size - 1) // batch_size\n",
    "for i in range(0, num_batches):\n",
    "    batch_ids = batching_ids[i:i + batch_size]\n",
    "    data = generate_data_parallel(batch_ids, A_dir, tgt_dir, num_workers=4)            # First 80 after shuffling\n",
    "    # print(f\"Batching {i} completed...\")\n",
    "    print(data)\n",
    "\n",
    "par_time = time.time() - start\n",
    "print(f\">> The not parallelized time is of: {not_par_time:.2f}, while the parallel time is of: {par_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e575ab56",
   "metadata": {},
   "source": [
    "Next, we define the GraphConv model along with the optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SECTION: Define-GNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "import torch.optim as optim\n",
    "\n",
    "class simple_gnn_gcn(nn.Module):\n",
    "    def __init__(self, num_node_features=1, hidden_channels=64,  num_predictions=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(1, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, 1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weights\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = torch.sigmoid(x)  # Outputs between 0-1\n",
    "        return x  # [num_nodes]\n",
    "    \n",
    "\n",
    "# Declare optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = simple_gnn_gcn(num_node_features=1, hidden_channels=72, num_predictions=1).to(device)\n",
    "loss_fn = nn.MSELoss()                                                # Loss function for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934c785",
   "metadata": {},
   "source": [
    "We then read traning sampled by batches and train the GraphConv model with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified code just for testing\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "tidx, batch_size, epochs = train_indices[:500], 100, 500\n",
    "num_batches = (len(tidx) + batch_size - 1) // batch_size\n",
    "start = time.time()\n",
    "model.train()\n",
    "\n",
    "# Load data method 2\n",
    "start = time.time()\n",
    "batch_size = 500\n",
    "batching_ids = data_ids.iloc[train_indices]\n",
    "num_batches = (len(batching_ids) + batch_size - 1) // batch_size\n",
    "for i in range(0, num_batches):\n",
    "    batch_ids = batching_ids[i:i + batch_size]\n",
    "    data = generate_data_parallel(batch_ids, A_dir, tgt_dir, num_workers=6)            # First 80 after shuffling\n",
    "    Dloader = DataLoader(data, batch_size=round(len(data)/10), shuffle=True)\n",
    "    print(f\"Batching {i}/{num_batches} completed...\")\n",
    "    # print(data)\n",
    "\n",
    "elpased_batching = time.time() - start\n",
    "print(f\"Time elapsed ({elpased_batching:.2f}) for loading {len(train_indices)} simulations.\")\n",
    "\n",
    "# Empty lists for predictions, targets, loss at each epoch\n",
    "epochs = 100\n",
    "x_train, y_train, loss_epochs  = [], [], []\n",
    "total_loss = 0\n",
    "for epoch in tqdm(range(0, epochs), total=epochs, desc=\"Training model:\"):\n",
    "    for data in Dloader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()   # Accumulate loss\n",
    "        x_train.append(out.cpu().detach().numpy()) \n",
    "        y_train.append(data.y.cpu().detach().numpy())\n",
    "    loss_epochs.append(total_loss)\n",
    "    \n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Elapsed time for batching and training with batch: {elapsed_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def train_batches(tidx, batch_size = 1000, epochs=500):\n",
    "    num_batches = (len(tidx) + batch_size - 1) // batch_size\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    # Empty lists for predictions, targets, loss at each epoch\n",
    "    x_train, y_train, loss_epochs  = [], [], []\n",
    "    for i in tqdm(range(0, len(tidx), batch_size), total=num_batches, desc=\"Loading batches\"):\n",
    "        data = [load_single_data(data_ids[id], A_dir, tgt_dir) for id in tidx[i:i + batch_size]]             # First 80 after shuffling\n",
    "        Dloaded = DataLoader(data, batch_size=round(len(data)/10), shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for data in Dloaded:\n",
    "                optimizer.zero_grad()\n",
    "                data = data.to(device)\n",
    "                out = model(data)\n",
    "                loss = loss_fn(out, data.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()   # Accumulate loss\n",
    "                if epoch==(epochs-1):\n",
    "                    x_train.append(out.cpu().detach().numpy()) \n",
    "                    y_train.append(data.y.cpu().detach().numpy())\n",
    "            loss_epochs.append(total_loss)\n",
    "            if epoch % 200 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {total_loss:.4f}\")\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Elapsed time for batching and training with batch: {elapsed_time:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
