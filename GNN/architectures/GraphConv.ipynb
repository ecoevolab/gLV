{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff921d0",
   "metadata": {},
   "source": [
    "This file was created for running pytorch learning with CUDA.\n",
    "\n",
    "## Documentation\n",
    "+ [ModuleList](https://docs.pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Define-GNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class simple_graphconv(nn.Module):\n",
    "    def __init__(self,  hidden_channels=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(1, hidden_channels) \n",
    "        self.conv2 = GraphConv(hidden_channels, 1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weights\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = torch.sigmoid(x)  # Outputs between 0-1\n",
    "        return x  # [num_nodes]\n",
    "\n",
    "class complex_graphconv(nn.Module):\n",
    "    def __init__(self,  hidden_channels=64, num_layers=10):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList() \n",
    "        self.convs.append(GraphConv(1, hidden_channels) )                           # First-convolution\n",
    "        # Hidden-convolutions\n",
    "        # The number of hidden layers is (num_layers - 2) due to the first and last layers\n",
    "        for _ in range(num_layers-2):\n",
    "            self.convs.append(GraphConv(hidden_channels, hidden_channels))          # Append-convolution\n",
    "        self.convs.append(GraphConv(hidden_channels, 1))                            # Final-convolution\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weights\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index, edge_weight)\n",
    "            if i != len(self.convs) - 1:        # not the last layer\n",
    "                x = F.relu(x)                   # Rectifier\n",
    "                # x = self.dropout(x)           # Dropout\n",
    "            else:\n",
    "                x = torch.sigmoid(x)  # Outputs between 0-1\n",
    "        return x  # [num_nodes]\n",
    "\n",
    "\n",
    "def seed_fn(seed=42):\n",
    "    # Set ALL seeds for full reproducibility\n",
    "    torch.manual_seed(seed)                 # Seed CPU \n",
    "    torch.cuda.manual_seed(seed)            # Seed GPU\n",
    "    np.random.seed(seed)                    # Seed numpy\n",
    "    random.seed(seed)                       # Seed python random\n",
    "    torch.backends.cudnn.deterministic = True   # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60ff37",
   "metadata": {},
   "source": [
    "# Complex model training function with Mixed Precision\n",
    "\n",
    "We implement a training function with mixed precision to optimize both performance and accuracy. The complex model incorporates several advanced training techniques:\n",
    "\n",
    "### Key Features\n",
    "\n",
    "[**Mixed Precision Training**](https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html)\n",
    "- Utilizes FP16 (half-precision) data types for forward pass.\n",
    "- Automatically scales gradients and converts them back to FP32 (full-precision) for backward pass to prevent underflow.\n",
    "\n",
    "[**Gradient Clipping**](https://github.com/pytorch/pytorch/blob/main/torch/nn/utils/clip_grad.py)\n",
    "- Scales gradient values by a specified threshold to maintain stable optimization.\n",
    "- The formula for `gradient_clipping` is:\n",
    "$\\text{grad} = \\text{grad} \\times \\min\\left(\\frac{\\text{max\\_norm}}{\\text{total\\_norm} + 10^{-6}}, 1\\right)$\n",
    "\n",
    "[**Adaptive learning rate**](https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html)\n",
    "- Implements `ReduceLROnPlateau` scheduler for adaptive learning rate adjustment.\n",
    "- Monitors training loss and reduces the learning rate by a factor when performance doesnt improve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8429030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.utils import clip_grad_norm_ as clip_grad\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch \n",
    "\n",
    "# COMPLEX MODEL\n",
    "# Declare data paths\n",
    "directory = '/home/mriveraceron/data/exp_20251125'\n",
    "train_files = glob.glob(f'{directory}/TrainBatch_*.pt')\n",
    "\n",
    "def training_loop(model, dat_batched, loss_fn, optimizer, epochs=100):\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    # Empty lists for predictions, targets, loss at each epoch\n",
    "    loss_history  =  []\n",
    "    total_elapsed = 0\n",
    "    # Testing ReduceLROnPlateau\n",
    "    # Reduce LR on Plateau scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',      # Monitor loss (we want it to go DOWN)\n",
    "        patience=10,     # Wait 10 epochs without improvement\n",
    "        factor=0.1,      # Reduce LR*factor\n",
    "        threshold=1e-5,  # Significant improvement\n",
    "    )\n",
    "    for iter in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "        epoch_loss = 0\n",
    "        for path in dat_batched:\n",
    "            data = torch.load(path, weights_only=False)          \n",
    "            loader = DataLoader(data, batch_size=50, shuffle=True)\n",
    "            for batch in loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                # TESTING MIXED PRECISION\n",
    "                #  Use 16-bytes for forward pass\n",
    "                with torch.autocast(device_type=device.type):\n",
    "                    out = model(batch)                          \n",
    "                    loss = loss_fn(out, batch.y)\n",
    "                epoch_loss += loss.item()   # Accumulate loss           \n",
    "                # Mixed precision backward pass\n",
    "                scaler.scale(loss).backward()               # Scale gradients UP (FP16 if  (< 6e-5) then becomes 0)\n",
    "                scaler.unscale_(optimizer)                  # Scale gradients back DOWN first\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   \n",
    "                # unscale the gradients of the optimizer's \n",
    "                scaler.step(optimizer)          \n",
    "                scaler.update()                   # Update scale for next iteration\n",
    "        # TESTING scheduler\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(epoch_loss)\n",
    "        # Section: Best loss\n",
    "        best_loss = float('inf') if iter == 1 else best_loss\n",
    "        best_loss = min(best_loss, epoch_loss)\n",
    "        best_epoch = iter if best_loss == epoch_loss else best_epoch\n",
    "        # Append epoch loss to history\n",
    "        loss_history.append(epoch_loss)\n",
    "        elapsed = time.time() - start\n",
    "        total_elapsed += elapsed\n",
    "        print(f\"Epoch {iter}: Loss = {loss},  Elapsed time: {elapsed:.2f}\")\n",
    "    # Summary\n",
    "    print(f'>> the total elapsed time with {epochs} epochs is {total_elapsed:.2f} seconds ( {total_elapsed/60:.2f} minutes)')      \n",
    "    return  loss_history, best_loss, best_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ecb33b",
   "metadata": {},
   "source": [
    "## Simple training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch \n",
    "\n",
    "# Declare data paths\n",
    "directory = '/home/mriveraceron/data/exp_20251125'\n",
    "train_files = glob.glob(f'{directory}/TrainBatch_*.pt')\n",
    "\n",
    "def training_loop(model, dat_batched, loss_fn, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    # Empty lists for predictions, targets, loss at each epoch\n",
    "    loss_history  =  []\n",
    "    total_elapsed = 0\n",
    "    for iter in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "        epoch_loss = 0\n",
    "        for path in dat_batched:\n",
    "            data = torch.load(path, weights_only=False)          \n",
    "            loader = DataLoader(data, batch_size=50, shuffle=True)\n",
    "            for batch in loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch)\n",
    "                loss = loss_fn(out, batch.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()   # Accumulate loss\n",
    "        # Section: Best loss\n",
    "        best_loss = float('inf') if iter == 1 else best_loss\n",
    "        best_loss = min(best_loss, epoch_loss)\n",
    "        best_epoch = iter if best_loss == epoch_loss else best_epoch\n",
    "        # Append epoch loss to history\n",
    "        loss_history.append(epoch_loss)\n",
    "        elapsed = time.time() - start\n",
    "        total_elapsed += elapsed\n",
    "        print(f\"Epoch {iter}: Loss = {loss},  Elapsed time: {elapsed:.2f}\")\n",
    "    # Summary\n",
    "    print(f'>> the total elapsed time with {epochs} epochs is {total_elapsed:.2f} seconds ( {total_elapsed/60:.2f} minutes)')      \n",
    "    return  loss_history, best_loss, best_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c9b83",
   "metadata": {},
   "source": [
    "## Plot loss over epochs\n",
    "\n",
    "We plot loss over epochs to observe if the model is learning something and how is it performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def loss_plotter(loss_epochs = None, epochs = None):\n",
    "    # After collecting your data\n",
    "    y = np.round(loss_epochs, 10)\n",
    "    x = list(range(0, epochs))\n",
    "    # Create scatter plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    # y = np.log1p(y)  # Log scale for better visualization\n",
    "    plt.plot(x, y, alpha=0.5)\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout() \n",
    "    plt.ylim(0, max(y))\n",
    "    plt.xlim(0, max(x))\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = loss_plotter(loss_train, epochs = 200)\n",
    "fig.savefig('/home/mriveraceron/glv-research/plots/complex-loss-GradientClipping.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e3752",
   "metadata": {},
   "source": [
    "# Validation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719494d",
   "metadata": {},
   "source": [
    "## Maximum keystoness for each community vs prediction\n",
    "\n",
    "This validation process evaluates the performance of a previously trained model by measuring its ability to predict maximum y values across different communities. The validation metric is expressed as a percentage accuracy.\n",
    "\n",
    "## Function used:\n",
    "+ Scatter: https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2336e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "def validation_fn(model, files, loss_fn, device):\n",
    "    model.eval()\n",
    "    # Assign variables\n",
    "    val_loss, total_graphs = 0, 0\n",
    "    true_idx, pred_idx = [], []\n",
    "    with torch.no_grad():\n",
    "        for path in files:\n",
    "            data_list = torch.load(path, weights_only=False)\n",
    "            total_graphs += len(data_list)\n",
    "            # Batch all graphs together\n",
    "            batch = Batch.from_data_list(data_list).to(device)\n",
    "            # Forward pass\n",
    "            out = model(batch)  # shape: [num_graphs * 30, features] or [num_graphs * 30]\n",
    "            loss = loss_fn(out, batch.y)\n",
    "            val_loss += loss.item()\n",
    "            # Process each graph separately\n",
    "            for i in range(batch.num_graphs):\n",
    "                mask = batch.batch == i  # nodes belonging to graph i\n",
    "                graph_y = batch.y[mask]  # 30 targets per graph\n",
    "                graph_out = out[mask]    # 30 predictions per graph  \n",
    "                # Get max index in 0-based\n",
    "                true_idx.append(torch.argmax(graph_y).item() + 1)\n",
    "                pred_idx.append(torch.argmax(graph_out).item() + 1)\n",
    "    return true_idx, pred_idx, val_loss, total_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c11d95",
   "metadata": {},
   "source": [
    "# Run both models\n",
    "\n",
    "We will run both models for performance testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77618dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, preds):\n",
    "    result = [a == b for a, b in zip(true, preds)]\n",
    "    correct = sum(result)\n",
    "    accuracy = (correct / len(true)) * 100\n",
    "    return(f'>> Validation Accuracy: {accuracy:.2f}% ({correct}/{len(true)})')\n",
    "\n",
    "# Define paths \n",
    "directory = '/home/mriveraceron/data/exp_20251125'\n",
    "train_files = glob.glob(f'{directory}/TrainBatch_*.pt')\n",
    "valid_files = glob.glob(f'{directory}/ValBatch_*.pt')\n",
    "loss_fn = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model 1\n",
    "seed_fn(38)\n",
    "model1 = simple_graphconv(hidden_channels=60).to(device)\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.01) \n",
    "mod1_loss, mod1_BestLoss, mod1_BestEpoch = training_loop(model1, train_files, loss_fn, optimizer, epochs=200)\n",
    "true_idx, pred_idx, mod1_ValLoss, total_graphs = validation_fn(model1, valid_files, loss_fn, device)\n",
    "pf1 = accuracy(true_idx, pred_idx)\n",
    "\n",
    "# Model 2\n",
    "seed_fn(38)\n",
    "model2 = complex_graphconv(hidden_channels=60, num_layers=5, dropout=0.5).to(device)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.01) \n",
    "mod2_loss, mod2_BestLoss, mod2_BestEpoch = training_loop(model2, train_files, loss_fn, optimizer, epochs=200)\n",
    "true_idx, pred_idx, mod2_ValLoss, total_graphs = validation_fn(model2, valid_files, loss_fn, device)\n",
    "pf2 = accuracy(true_idx, pred_idx)\n",
    "\n",
    "print(f'Simple model performance. {pf1} \\n Complex model performance. {pf2}')\n",
    "\n",
    "# Create scatter plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "epochs = 200\n",
    "x = list(range(0, epochs))\n",
    "# y = np.log1p(y)  # Log scale for better visualization\n",
    "plt.plot(x, mod1_loss, alpha=0.5, color='green', label='Simple') \n",
    "plt.plot(x, mod2_loss, alpha=0.5, color='red', label='Mixed-precision') \n",
    "# Add labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout() \n",
    "plt.ylim(0, max(max(mod1_loss), max(mod2_loss)))\n",
    "plt.xlim(0, 200)\n",
    "plt.legend(loc='upper right')\n",
    "fig.savefig('/home/mriveraceron/glv-research/plots/models-20251125.png', dpi=150, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2b8ce",
   "metadata": {},
   "source": [
    "# Understanding pytorch Dataloader\n",
    "\n",
    "\n",
    "The following code demonstrates how to get the maximum value of the y tensor for each individual sample (graph) when using DataLoader for batching in PyTorch Geometric.\n",
    "\n",
    "## Problem 1\n",
    "When graphs are batched together using DataLoader, all node features and labels are concatenated. The `batch` vector indicates which nodes belong to which graph, but we need to extract the maximum y value for each original graph separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "# Original Data objects:\n",
    "dataset = []\n",
    "for i in range(5):\n",
    "    data = Data(x=torch.randn(10, 1), y=torch.tensor([1, 1, 100, 1, 1, 1, 1, 1, 1, (i+1)*(10**(i+1))]))\n",
    "    dataset.append(data)\n",
    "\n",
    "# Create DataLoader\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# How batches work\n",
    "batch = Batch.from_data_list(dataset)                   # Create batch\n",
    "print(f\"batch.batch: {batch.batch}\")                    # To which graph each node belongs \n",
    "print(f\"batch.y: {batch.y}\")                            # All y values concatenated\n",
    "print(f\"Total nodes: {batch.num_nodes}\")                # Number of nodes in the batch\n",
    "print(f\"Number of graphs: {batch.num_graphs}\")          # Number of graphs in the batch\n",
    "\n",
    "# Iterate through batches\n",
    "vector = []\n",
    "for batch in loader:\n",
    "    print(f\"Graphs per batch: {batch.num_graphs}\")\n",
    "    print(f\"Total nodes in batch: {batch.num_nodes}\")   # x are the nodes\n",
    "    print(f\"x (nodes) shape: {batch.x.shape}\")\n",
    "    print(f\"y shape: {batch.y.shape}\")\n",
    "    print(f\"batch vector: {batch.batch}\")\n",
    "    # print(f'testing: {batch.y.unsqueeze(-1)}')\n",
    "    max_y_per_graph = global_max_pool(batch.y.unsqueeze(-1), batch.batch).squeeze(-1)\n",
    "    vector.extend( max_y_per_graph.tolist() )\n",
    "    print(f\"Max y per graph: {max_y_per_graph}\")\n",
    "    print(\"---\")\n",
    "\n",
    "sorted(vector)\n",
    "\n",
    "\n",
    "# How does unsqueeze work?\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "x.size()\n",
    "x = torch.unsqueeze(x, -1)\n",
    "x.size()\n",
    "x = torch.squeeze(x, -1)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a512879",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "When batching graphs, all node features and targets are concatenated into a single large tensor. To aggregate node-level predictions back to graph-level predictions, we use the `scatter` function, which groups nodes by their graph assignment using the `batch.batch` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96112a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([0, 1, 0, 2, 1, 2])\n",
    "data = torch.tensor([10, 20, 30, 40, 50, 60])\n",
    "scatter(data, index, dim=0, reduce='max')  # Should be [30, 50, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b54f8",
   "metadata": {},
   "source": [
    "## Poblem 3\n",
    "\n",
    "We are interested in obtaining the index of the ith element whose value is the maximum in the tensor. We can do such with `torch.argmax`. We can do such in individual tensors but when using batches from Dataloader we will use a different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2beea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Use on individual tensor\n",
    "a = torch.randn(4)\n",
    "torch.argmax(a)\n",
    "\n",
    "# Usage on batched data\n",
    "dataset = []\n",
    "for i in range(5):\n",
    "    data = Data(x=torch.randn(10, 1), y=torch.randn(10) )\n",
    "    dataset.append(data)\n",
    "\n",
    "# Create DataLoader\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate through batches\n",
    "vector = []\n",
    "for g in dataset:\n",
    "    idx_max = torch.argmax(g.y).numpy().item()\n",
    "    print(f'>> y tensor: {g.y}')\n",
    "    vector.append( idx_max + 1 )\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b881b",
   "metadata": {},
   "source": [
    "# Multiple seeds testing\n",
    "\n",
    "As initial weight matrix is completely random, testing for multiple seeds and the choosing of the best one has to be tested. For this purpose we generate some seeds and test for improvement in loss over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def seed_fn(seed=42):\n",
    "    # Set ALL seeds for full reproducibility\n",
    "    torch.manual_seed(seed)                 # Seed CPU \n",
    "    torch.cuda.manual_seed(seed)            # Seed GPU\n",
    "    np.random.seed(seed)                    # Seed numpy\n",
    "    random.seed(seed)                       # Seed python random\n",
    "    torch.backends.cudnn.deterministic = True   # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n",
    "def test_seeds(nseeds=10):\n",
    "    # Generate 10 random integers between 1 and 10^10\n",
    "    random_nums = np.random.randint(1, int(1e5) + 1, size=nseeds)\n",
    "    print(random_nums)\n",
    "    results = []\n",
    "    for s in random_nums:\n",
    "        seed_fn(s)\n",
    "        model = simple_gnn_gcn(hidden_channels=60).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "        x_train, y_train, loss_train = training_loop(model, train_files, loss_fn, optimizer, epochs=100)\n",
    "        x_val, y_val, loss_val = validation_fn(model, valid_files, loss_fn, device)\n",
    "        # Performance metrics\n",
    "        mse = mean_squared_error(y_val, x_val)\n",
    "        r2 = r2_score(y_val, x_val)\n",
    "        # Append results\n",
    "        results.append({'seed': s, \n",
    "                        'val_mse': round(mse, 4),\n",
    "                        'val_r2': round(r2, 4),\n",
    "                        'mean_trloss': round(np.mean(loss_train), 4)\n",
    "                })\n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        del optimizer\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache\n",
    "    return results\n",
    "\n",
    "tmp = test_seeds(nseeds=5)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c434816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_plotter(preds = None, tgts = None, path = None ):\n",
    "    # After collecting your data\n",
    "    preds = np.concatenate(preds)  # predictions\n",
    "    tgts = np.concatenate(tgts)  # targets\n",
    "    # Create scatter plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(preds, tgts, alpha=0.5)\n",
    "   # Add perfect prediction line (y=x)\n",
    "    plt.plot([0,  np.max(tgts)], [0,  np.max(tgts)], 'r--', label='Perfect prediction')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('True Values')\n",
    "    plt.title('Predictions vs True Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, max(tgts))\n",
    "    plt.xlim(0, max(tgts))\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
