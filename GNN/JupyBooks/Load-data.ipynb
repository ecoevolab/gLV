{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c464b63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Section: Generate-paths\n",
    "exp_dir = \"/mnt/data/sur/users/mrivera/Train-sims/4379fd40-9f0a\"\n",
    "A_dir = os.path.join(exp_dir, \"A-mat\")\n",
    "tgt_dir = os.path.join(exp_dir, \"GNN-targets\")\n",
    "odes_path = os.path.join(exp_dir, \"raw-ODEs\")\n",
    "data_path = os.path.join(exp_dir, \"parameters-sims.tsv\")\n",
    "\n",
    "# Generate ID for training.\n",
    "timeID = datetime.now().strftime(\"Y%YM%mD%d\")\n",
    "\n",
    "#  Load-data\n",
    "data_ids = pd.read_csv(data_path, sep=\"\\t\", usecols=['id'])['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Load-function\n",
    "from torch_geometric.data import Data\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "def load_single_data(id, A_dir, tgt_path):\n",
    "    # Load adjacency matrix \n",
    "    A_path = os.path.join(A_dir, f\"A_{id}.feather\")\n",
    "    A = pd.read_feather(A_path).to_numpy(dtype=np.float32)\n",
    "    # Vector of edge weights\n",
    "    row_idx, col_idx = np.nonzero(A)\n",
    "    edge_weights = A[row_idx, col_idx]\n",
    "    # Convert to torch tensors efficiently\n",
    "    edge_index = torch.from_numpy(np.vstack([row_idx, col_idx]).astype(np.int64))\n",
    "    edge_weights = torch.from_numpy(edge_weights)\n",
    "    # Load target features \n",
    "    tgt_path = os.path.join(tgt_dir, f\"tgt_{id}.feather\")\n",
    "    tgt_table =  feather.read_table(tgt_path, columns=['K_s'])\n",
    "    y_tensor = torch.from_numpy(tgt_table.to_pandas().to_numpy(dtype=np.float32))   \n",
    "    # Node features - simple ones vector\n",
    "    n = A.shape[0]\n",
    "    x_tensor = torch.ones(n, 1, dtype=torch.float32)\n",
    "    # Clean up large intermediate\n",
    "    del A, tgt_table\n",
    "    # Create Data object\n",
    "    data = Data(\n",
    "        x=x_tensor,\n",
    "        edge_weights=edge_weights,\n",
    "        edge_index=edge_index,\n",
    "        y=y_tensor\n",
    "    )\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Divide-data\n",
    "import random\n",
    "\n",
    "# Load all data samples (for demo, we use only first 100 samples)\n",
    "indices = list(range(1, len(data_ids)))  # Indices 1-100\n",
    "random.shuffle(indices)  # Uses Python's random module (already seeded)\n",
    "\n",
    "# Now select first 80 for training, rest for validation\n",
    "indx = round(len(indices) * .8)\n",
    "train_indices = indices[:indx]            # First 80 shuffled indices\n",
    "val_indices = indices[indx:]              # Last 20 shuffled indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3cfa2",
   "metadata": {},
   "source": [
    "## Performance Comparison: Sequential vs Parallel Data Loading\n",
    "\n",
    "Compare loading times between `load_single_data` (sequential) and `generate_data_parallel` (parallel processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c87ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Declare new function\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def generate_data_parallel(idx, A_dir, tgt_dir, num_workers=4):  # idx is a list\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        data_list = list(executor.map(\n",
    "            load_single_data,           \n",
    "            idx,                       # List of IDs to iterate over\n",
    "            [A_dir]*len(idx),          # Repeat A_dir for each ID\n",
    "            [tgt_dir]*len(idx)         # Repeat tgt_dir for each ID\n",
    "        ))\n",
    "    return data_list\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Generate data with method 1\n",
    "x = train_indices[:1000]\n",
    "\n",
    "start = time.time()\n",
    "data = [load_single_data(data_ids[id], A_dir, tgt_dir) for id in x] \n",
    "not_par_time = time.time() - start\n",
    "\n",
    "# Generate data with method 2\n",
    "start = time.time()\n",
    "batch_size = 100\n",
    "batching_ids = data_ids[x]\n",
    "num_batches = (len(batching_ids) + batch_size - 1) // batch_size\n",
    "for i in range(0, num_batches):\n",
    "    batch_ids = batching_ids[i:i + batch_size]\n",
    "    data = generate_data_parallel(batch_ids, A_dir, tgt_dir, num_workers=4)            # First 80 after shuffling\n",
    "    # print(f\"Batching {i} completed...\")\n",
    "    print(data)\n",
    "\n",
    "par_time = time.time() - start\n",
    "print(f\">> The not parallelized time is of: {not_par_time:.2f}, while the parallel time is of: {par_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e024d",
   "metadata": {},
   "source": [
    "## Parallel Data Loading of training dataset and save files\n",
    "\n",
    "Load data using parallelize function `generate_data_parallel`. We then save data in batches at a specific directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a18a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def generate_data_parallel(idx, A_dir, tgt_dir, num_workers=4):  # idx is a list\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        data_list = list(executor.map(\n",
    "            load_single_data,           \n",
    "            idx,                       # List of IDs to iterate over\n",
    "            [A_dir]*len(idx),          # Repeat A_dir for each ID\n",
    "            [tgt_dir]*len(idx)         # Repeat tgt_dir for each ID\n",
    "        ))\n",
    "    return data_list\n",
    "\n",
    "# Generate data and separate in batches\n",
    "path = '/mnt/data/sur/users/mrivera/Train-sims/4379fd40-9f0a/batching'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "start = time.time()\n",
    "batch_size = 1000\n",
    "batching_ids = data_ids[train_indices] #fixme\n",
    "num_batches = (len(batching_ids) + batch_size - 1) // batch_size\n",
    "for i in range(0, num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_ids = data_ids[start_idx:end_idx]\n",
    "    # Load data\n",
    "    data = generate_data_parallel(batch_ids, A_dir, tgt_dir, num_workers=6)\n",
    "    name = os.path.join(path, f'TrainBatch_{i}.pt')\n",
    "    torch.save(data, name)  # Actually save the data\n",
    "    del data  # Free memory immediately\n",
    "    print(f\"Saved batch {i}/{num_batches-1}: {name}\")\n",
    "\n",
    "par_time = time.time() - start\n",
    "print(f\">> The batch size is of {batch_size:.2f}, the number of batches was {num_batches}, while the elapsed time is of: {par_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batch\n",
    "# Add PyG Data objects to safe globals\n",
    "torch.serialization.add_safe_globals([Data])\n",
    "\n",
    "# Now load normally\n",
    "start = time.time()\n",
    "for i in range(0, num_batches):\n",
    "    data= torch.load(f'/mnt/data/sur/users/mrivera/Train-sims/4379fd40-9f0a/batching/BatchedData_{i}.pt',  weights_only=False)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\">> The elapsed time is {elapsed:.2f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e1e4c",
   "metadata": {},
   "source": [
    "## Parallel Data Loading of validation dataset and save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same idea just for validation\n",
    "\n",
    "start = time.time()\n",
    "batch_size = 1000\n",
    "batching_ids = data_ids[val_indices] #fixme\n",
    "num_batches = (len(batching_ids) + batch_size - 1) // batch_size\n",
    "for i in range(0, num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_ids = data_ids[start_idx:end_idx]\n",
    "    # Load data\n",
    "    data = generate_data_parallel(batch_ids, A_dir, tgt_dir, num_workers=6)\n",
    "    name = os.path.join(path, f'ValBatch_{i}.pt')\n",
    "    torch.save(data, name)  # Actually save the data\n",
    "    del data  # Free memory immediately\n",
    "    print(f\"Saved batch {i}/{num_batches-1}: {name}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
